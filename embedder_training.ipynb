{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "063eb57a-fcb7-4541-9591-47918f2ffbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a058eb3-4ff1-4804-b86b-70488987c30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87f954e8-9d96-4704-9813-e06537bf6363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download script\n",
      "Downloading datasets\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"datasets/DATASET-NAME\"):\n",
    "    print(\"Starting download script\")\n",
    "    output = subprocess.check_output([\"python\", \"download_datasets.py\"])\n",
    "    print(output.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8da7eb-c840-423d-a502-bd43c18e7335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930205a-4355-40bb-821c-7318b9e15e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f5dce7-1d26-42fa-9dc4-79d906521169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1df5007d-532d-4ebe-97f8-224bd4225734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc131f16-4096-4211-aa2c-32169d25deeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paws_wiki',\n",
       " 'paws_x_wiki',\n",
       " 'salient_span_wikipedia',\n",
       " 'wiki40b',\n",
       " 'wiki_auto',\n",
       " 'wiki_bio',\n",
       " 'wiki_dialog',\n",
       " 'wiki_table_questions',\n",
       " 'wiki_table_text',\n",
       " 'wikiann',\n",
       " 'wikihow',\n",
       " 'wikipedia',\n",
       " 'wikipedia_toxicity_subtypes']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in tfds.list_builders() if 'wiki' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd842bbd-2db6-46f2-ba28-10c26f7f5302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = tfds.load('wiki_auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76733778-f020-4399-b704-bf7df71cb89f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7140888b-f060-40b1-b006-4150e903995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = tfds.builder('wiki_auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2dfed151-f1f6-4a49-9357-3a33348a6927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the tfrecord files (no-op if already exists)\n",
    "builder.download_and_prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91696961-6e30-454d-b67f-eb1ddae2de45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dev': <_PrefetchDataset element_spec={'GLEU-score': TensorSpec(shape=(), dtype=tf.float64, name=None), 'alignment_label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'normal_sentence': TensorSpec(shape=(), dtype=tf.string, name=None), 'normal_sentence_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'simple_sentence': TensorSpec(shape=(), dtype=tf.string, name=None), 'simple_sentence_id': TensorSpec(shape=(), dtype=tf.string, name=None)}>,\n",
       " 'test': <_PrefetchDataset element_spec={'GLEU-score': TensorSpec(shape=(), dtype=tf.float64, name=None), 'alignment_label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'normal_sentence': TensorSpec(shape=(), dtype=tf.string, name=None), 'normal_sentence_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'simple_sentence': TensorSpec(shape=(), dtype=tf.string, name=None), 'simple_sentence_id': TensorSpec(shape=(), dtype=tf.string, name=None)}>}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Load the `tf.data.Dataset`\n",
    "dataset = builder.as_dataset()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "768385e9-477b-4bf9-afd9-34bf5492013e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec={'GLEU-score': TensorSpec(shape=(), dtype=tf.float64, name=None), 'alignment_label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'normal_sentence': TensorSpec(shape=(), dtype=tf.string, name=None), 'normal_sentence_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'simple_sentence': TensorSpec(shape=(), dtype=tf.string, name=None), 'simple_sentence_id': TensorSpec(shape=(), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devset = dataset['dev']\n",
    "devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b6ed694-a8e1-4364-8bef-97388c709abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TakeDataset element_spec={'GLEU-score': TensorSpec(shape=(), dtype=tf.float64, name=None), 'alignment_label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'normal_sentence': TensorSpec(shape=(), dtype=tf.string, name=None), 'normal_sentence_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'simple_sentence': TensorSpec(shape=(), dtype=tf.string, name=None), 'simple_sentence_id': TensorSpec(shape=(), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = devset.take(1)  # Only take a single example\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "def4b1e1-921f-44cf-bda6-1e5c6a9e1204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GLEU-score': <tf.Tensor: shape=(), dtype=float64, numpy=0.029702970758080482>,\n",
       " 'alignment_label': <tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " 'normal_sentence': <tf.Tensor: shape=(), dtype=string, numpy=b'For example, in the petroleum industry, sodium hydroxide is used as an additive in drilling mud to increase alkalinity in bentonite mud systems, to increase the mud viscosity, and to neutralize any acid gas (such as hydrogen sulfide and carbon dioxide) which may be encountered in the geological formation as drilling progresses.'>,\n",
       " 'normal_sentence_id': <tf.Tensor: shape=(), dtype=string, numpy=b'375_39549-1-34-0'>,\n",
       " 'simple_sentence': <tf.Tensor: shape=(), dtype=string, numpy=b'Sodium hydroxide can be made (with chlorine and hydrogen) using the chloralkali process.'>,\n",
       " 'simple_sentence_id': <tf.Tensor: shape=(), dtype=string, numpy=b'375_39549-0-3-0'>}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = example.get_single_element()\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0bfae779-1a2b-4106-a78a-602e582a3777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'For example, in the petroleum industry, sodium hydroxide is used as an additive in drilling mud to increase alkalinity in bentonite mud systems, to increase the mud viscosity, and to neutralize any acid gas (such as hydrogen sulfide and carbon dioxide) which may be encountered in the geological formation as drilling progresses.'>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e['normal_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c567b-e400-4dec-a450-8fc207d755dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8284835c-bab6-46a9-aac0-e1dd791c703c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ce789-2a9c-428d-afb3-558084d21e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1fbd549-2337-4bd3-a0b7-ee3db61f589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the desired size of the subset in bytes (e.g., 1GB)\n",
    "DESIRED_SUBSET_SIZE = 1 * 1024 * 1024 * 1024  # 1GB in bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a39ee95-8f22-4111-94c5-90422055fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_wikipedia_dataset():\n",
    "#     # Load the Wikipedia dataset\n",
    "#     ds, ds_info = tfds.load('wikipedia/20200301.en', split='train', with_info=True)\n",
    "\n",
    "#     # Information about the dataset\n",
    "#     print(ds_info)\n",
    "\n",
    "#     return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809b7c7f-d1a8-4279-b6b3-9e8007e77670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_and_save_subset(dataset, desired_size):\n",
    "    current_size = 0\n",
    "    subset_data = []\n",
    "\n",
    "    for example in tfds.as_numpy(dataset):\n",
    "        text = example['text']\n",
    "        current_size += len(text)\n",
    "\n",
    "        # Add the text to your subset\n",
    "        subset_data.append(text)\n",
    "\n",
    "        # Check if the current size exceeds the desired size\n",
    "        if current_size >= desired_size:\n",
    "            break\n",
    "\n",
    "    # Save the subset data to a file\n",
    "    with open('wikipedia_subset.txt', 'w', encoding='utf-8') as file:\n",
    "        for item in subset_data:\n",
    "            file.write(item.decode('utf-8') + '\\n')\n",
    "\n",
    "    print(f\"Subset created with size: {current_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f77be435-6c0e-4259-9956-582828e87fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\David\\tensorflow_datasets\\wikipedia\\20200301.en\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:21, ? file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 0 MiB [00:21, ? MiB/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:21<?, ? url/s]\n"
     ]
    },
    {
     "ename": "DownloadError",
     "evalue": "Failed to get url https://dumps.wikimedia.your.org/enwiki/20200301/dumpstatus.json. HTTP code: 404.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDownloadError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Download the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m wikipedia_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_wikipedia_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m, in \u001b[0;36mdownload_wikipedia_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_wikipedia_dataset\u001b[39m():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Load the Wikipedia dataset\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     ds, ds_info \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwikipedia/20200301.en\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Information about the dataset\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ds_info)\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:168\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:642\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \n\u001b[0;32m    525\u001b[0m \u001b[38;5;124;03m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;124;03m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    636\u001b[0m dbuilder \u001b[38;5;241m=\u001b[39m _fetch_builder(\n\u001b[0;32m    637\u001b[0m     name,\n\u001b[0;32m    638\u001b[0m     data_dir,\n\u001b[0;32m    639\u001b[0m     builder_kwargs,\n\u001b[0;32m    640\u001b[0m     try_gcs,\n\u001b[0;32m    641\u001b[0m )\n\u001b[1;32m--> 642\u001b[0m \u001b[43m_download_and_prepare_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    645\u001b[0m   as_dataset_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:501\u001b[0m, in \u001b[0;36m_download_and_prepare_builder\u001b[1;34m(dbuilder, download, download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m    500\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 501\u001b[0m   dbuilder\u001b[38;5;241m.\u001b[39mdownload_and_prepare(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs)\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:168\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:691\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[0;32m    689\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mread_from_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir)\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 691\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    696\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[0;32m    697\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[0;32m    698\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[0;32m    699\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[0;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1547\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1546\u001b[0m   optional_pipeline_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1547\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_generators(  \u001b[38;5;66;03m# pylint: disable=unexpected-keyword-arg\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     dl_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptional_pipeline_kwargs\n\u001b[0;32m   1549\u001b[0m )\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m split_builder\u001b[38;5;241m.\u001b[39mnormalize_legacy_split_generators(\n\u001b[0;32m   1554\u001b[0m     split_generators\u001b[38;5;241m=\u001b[39msplit_generators,\n\u001b[0;32m   1555\u001b[0m     generator_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_examples,\n\u001b[0;32m   1556\u001b[0m     is_beam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, BeamBasedBuilder),\n\u001b[0;32m   1557\u001b[0m )\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\tensorflow_datasets\\text\\wikipedia.py:459\u001b[0m, in \u001b[0;36mWikipedia._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m    457\u001b[0m info_url \u001b[38;5;241m=\u001b[39m _base_url(lang) \u001b[38;5;241m+\u001b[39m _INFO_FILE\n\u001b[0;32m    458\u001b[0m \u001b[38;5;66;03m# Use dictionary since testing mock always returns the same result.\u001b[39;00m\n\u001b[1;32m--> 459\u001b[0m downloaded_files \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minfo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo_url\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m xml_urls \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    462\u001b[0m total_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:688\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_downloader\u001b[38;5;241m.\u001b[39mtqdm():\n\u001b[0;32m    687\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extractor\u001b[38;5;241m.\u001b[39mtqdm():\n\u001b[1;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_promise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_extract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:831\u001b[0m, in \u001b[0;36m_map_promise\u001b[1;34m(map_fn, all_inputs)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[0;32m    828\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    829\u001b[0m     map_fn, all_inputs\n\u001b[0;32m    830\u001b[0m )  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[1;32m--> 831\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtree_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_promises\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\tree\\__init__.py:435\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m--> 435\u001b[0m                     [func(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\tree\\__init__.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m--> 435\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:832\u001b[0m, in \u001b[0;36m_map_promise.<locals>.<lambda>\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[0;32m    828\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    829\u001b[0m     map_fn, all_inputs\n\u001b[0;32m    830\u001b[0m )  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[0;32m    831\u001b[0m res \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m--> 832\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, all_promises\n\u001b[0;32m    833\u001b[0m )  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\promise\\promise.py:512\u001b[0m, in \u001b[0;36mPromise.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    510\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target()\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(timeout \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_TIMEOUT)\n\u001b[1;32m--> 512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\promise\\promise.py:516\u001b[0m, in \u001b[0;36mPromise._target_settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_target_settled_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, _raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# type: (bool) -> Any\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\promise\\promise.py:226\u001b[0m, in \u001b[0;36mPromise._settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _raise:\n\u001b[0;32m    225\u001b[0m     raise_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fulfillment_handler0\n\u001b[1;32m--> 226\u001b[0m     \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_traceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fulfillment_handler0\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\six.py:719\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    721\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\promise\\promise.py:844\u001b[0m, in \u001b[0;36m_process_future_result.<locals>.handle_future_result\u001b[1;34m(future)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandle_future_result\u001b[39m(future):\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;66;03m# type: (Any) -> None\u001b[39;00m\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 844\u001b[0m         resolve(\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    846\u001b[0m         tb \u001b[38;5;241m=\u001b[39m exc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\concurrent\\futures\\_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\tensorflow_datasets\\core\\download\\downloader.py:231\u001b[0m, in \u001b[0;36m_Downloader._sync_download\u001b[1;34m(self, url, destination_path, verify)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnimplementedError:\n\u001b[0;32m    229\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_url(url, verify\u001b[38;5;241m=\u001b[39mverify) \u001b[38;5;28;01mas\u001b[39;00m (response, iter_content):\n\u001b[0;32m    232\u001b[0m   fname \u001b[38;5;241m=\u001b[39m _get_filename(response)\n\u001b[0;32m    233\u001b[0m   path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(destination_path, fname)\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\tensorflow_datasets\\core\\download\\downloader.py:304\u001b[0m, in \u001b[0;36m_open_with_requests\u001b[1;34m(url, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m   url \u001b[38;5;241m=\u001b[39m _normalize_drive_url(url)\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m--> 304\u001b[0m   \u001b[43m_assert_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m   \u001b[38;5;28;01myield\u001b[39;00m (response, response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mio\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE))\n",
      "File \u001b[1;32mF:\\anaconda3\\envs\\p39_v2\\lib\\site-packages\\tensorflow_datasets\\core\\download\\downloader.py:331\u001b[0m, in \u001b[0;36m_assert_status\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ensure the URL response is 200.\"\"\"\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m--> 331\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m download_utils_lib\u001b[38;5;241m.\u001b[39mDownloadError(\n\u001b[0;32m    332\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to get url \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. HTTP code: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    333\u001b[0m           response\u001b[38;5;241m.\u001b[39murl, response\u001b[38;5;241m.\u001b[39mstatus_code\n\u001b[0;32m    334\u001b[0m       )\n\u001b[0;32m    335\u001b[0m   )\n",
      "\u001b[1;31mDownloadError\u001b[0m: Failed to get url https://dumps.wikimedia.your.org/enwiki/20200301/dumpstatus.json. HTTP code: 404."
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "wikipedia_dataset = download_wikipedia_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0ae40-c395-4110-abb7-555b80b2ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and save a subset of the dataset\n",
    "process_and_save_subset(wikipedia_dataset, DESIRED_SUBSET_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
