{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "62016cab48535386",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T04:58:17.392991Z",
     "start_time": "2023-12-22T04:58:01.225926200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "x = 10\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2efb6c78281ceaef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T04:58:18.851162500Z",
     "start_time": "2023-12-22T04:58:17.396048200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "41767dc5a46f289d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T04:58:18.859209900Z",
     "start_time": "2023-12-22T04:58:18.846090500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda is available\")\n",
    "else:\n",
    "    if 'cpu' in torch.__version__:\n",
    "        print(\"NOT using cuda, torch version is +cpu instead of +cu\")\n",
    "    else:\n",
    "        print(\"NOT using cuda, assert torch.cuda.is_available() failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ae3e77c0437e9bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T04:58:18.987461700Z",
     "start_time": "2023-12-22T04:58:18.882721500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None, verbose = False) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"Forward pass now ~~~~~~~~~~~~~~~~\")\n",
    "            print(\"Shape of input:\", src.shape)\n",
    "            print(src)\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        if verbose:\n",
    "            print(\"Shape after embedding:\", src.shape)\n",
    "            print(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        if verbose:\n",
    "            print(\"Shape after positional encoding:\", src.shape)\n",
    "            print(src)\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
    "        if verbose:\n",
    "            print(\"Shape of mask:\", src_mask.shape)\n",
    "            print(src_mask)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        if verbose:\n",
    "            print(\"Shape after transformer encoding:\", output.shape)\n",
    "            print(output)\n",
    "        output = self.linear(output)\n",
    "        if verbose:\n",
    "            print(\"Shape after linear:\", output.shape)\n",
    "            print(output)\n",
    "        return output\n",
    "    \n",
    "    def __call__(self, src: Tensor, src_mask: Tensor = None, verbose = False) -> Tensor:\n",
    "        return self.forward(src, src_mask = None, verbose = verbose)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7f6a54cef4bb8cde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T04:58:19.014460500Z",
     "start_time": "2023-12-22T04:58:18.912720200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        # print(\"PE: Shape of input to PosEnc.forward():\", x.shape)\n",
    "        # print(\"PE: Shape of self.pe[:x.size(0)]:\", self.pe[:x.size(0)].shape)\n",
    "        x = x + self.pe[:x.size(0)].squeeze(1)\n",
    "        # print(\"PE: Shape of output after adding PE:\", x.shape)\n",
    "        # print(\"PE: Shape of output after dropout:\", self.dropout(x).shape)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return self.forward(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d886062fb778dff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T04:58:19.017493500Z",
     "start_time": "2023-12-22T04:58:18.920783900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %pip install portalocker\n",
    "# %pip install torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7febc57dbb36b90a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T04:58:19.037463800Z",
     "start_time": "2023-12-22T04:58:18.940150100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dca593c316a53bed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T04:58:27.164372200Z",
     "start_time": "2023-12-22T04:58:18.982465200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size, seq_len:  20 102499\n",
      "batch size, seq_len:  10 21441\n",
      "batch size, seq_len:  10 24185\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# ``train_iter`` was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    print('batch size, seq_len: ', bsz, seq_len)\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape ``[seq_len, batch_size]``\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d803acffc2ec94f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T04:58:31.078717800Z",
     "start_time": "2023-12-22T04:58:27.180908500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2049990"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = data_process(train_iter)\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "val_data = data_process(val_iter)\n",
    "val_data = val_data.to(device)\n",
    "\n",
    "test_data = data_process(test_iter)\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d88d2c892cda70c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T04:58:31.139456500Z",
     "start_time": "2023-12-22T04:58:31.067470500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   9, 3849, 3869,  ..., 2442, 4810,    3], device='cuda:0')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e11b24eabd79dae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T04:58:31.172462300Z",
     "start_time": "2023-12-22T04:58:31.105238300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bptt = 256\n",
    "# bptt = 1024\n",
    "bptt = 2048\n",
    "# bptt = 3072\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4d6bf4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, scheduler = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8293d59742313d7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T14:43:43.635007100Z",
     "start_time": "2023-12-22T14:43:37.773007100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "train_accelerate_factor = 1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model: nn.Module, verbose=False) -> None:\n",
    "    \n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 100\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    print('batch size = ', bptt, '; number of batches = ', num_batches)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        if batch % train_accelerate_factor != 0:\n",
    "            continue\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        output = model(data, verbose=verbose)\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "    \n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            start_time = time.time()\n",
    "            cur_loss = total_loss / log_interval\n",
    "            train_losses.append(cur_loss)\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "\n",
    "    return train_losses\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "            # total_loss += criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "473190a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 50  # embedding dimension (default 200)\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b6a14b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.25  # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bd4844ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model():\n",
    "    model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "079675e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler = new_model()\n",
    "\n",
    "data, targets = get_batch(train_data, 0)\n",
    "data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "output = model(data, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a0e82ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch   0 |     0/ 1000 batches | lr 0.25 | ms/batch  0.54 | loss  0.10 | ppl     1.11\n",
      "| epoch   0 |   100/ 1000 batches | lr 0.25 | ms/batch  0.70 | loss  0.25 | ppl     1.29\n",
      "| epoch   0 |   200/ 1000 batches | lr 0.25 | ms/batch  0.72 | loss  0.34 | ppl     1.40\n",
      "| epoch   0 |   300/ 1000 batches | lr 0.25 | ms/batch  0.70 | loss  0.25 | ppl     1.28\n",
      "| epoch   0 |   400/ 1000 batches | lr 0.25 | ms/batch  0.71 | loss  0.30 | ppl     1.35\n",
      "| epoch   0 |   500/ 1000 batches | lr 0.25 | ms/batch  0.66 | loss  0.23 | ppl     1.26\n",
      "| epoch   0 |   600/ 1000 batches | lr 0.25 | ms/batch  0.61 | loss  0.18 | ppl     1.20\n",
      "| epoch   0 |   700/ 1000 batches | lr 0.25 | ms/batch  0.61 | loss  0.18 | ppl     1.20\n",
      "| epoch   0 |   800/ 1000 batches | lr 0.25 | ms/batch  0.61 | loss  0.17 | ppl     1.19\n",
      "| epoch   0 |   900/ 1000 batches | lr 0.25 | ms/batch  0.61 | loss  0.17 | ppl     1.18\n",
      "| epoch   0 |  1000/ 1000 batches | lr 0.25 | ms/batch  0.66 | loss  0.16 | ppl     1.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time:  2.29s | valid loss  8.17 | valid ppl  3537.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch   1 |     0/ 1000 batches | lr 0.24 | ms/batch  0.30 | loss  0.08 | ppl     1.09\n",
      "| epoch   1 |   100/ 1000 batches | lr 0.24 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch   1 |   200/ 1000 batches | lr 0.24 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch   1 |   300/ 1000 batches | lr 0.24 | ms/batch  0.59 | loss  0.15 | ppl     1.16\n",
      "| epoch   1 |   400/ 1000 batches | lr 0.24 | ms/batch  0.65 | loss  0.15 | ppl     1.17\n",
      "| epoch   1 |   500/ 1000 batches | lr 0.24 | ms/batch  0.71 | loss  0.15 | ppl     1.16\n",
      "| epoch   1 |   600/ 1000 batches | lr 0.24 | ms/batch  0.63 | loss  0.17 | ppl     1.18\n",
      "| epoch   1 |   700/ 1000 batches | lr 0.24 | ms/batch  0.60 | loss  0.16 | ppl     1.17\n",
      "| epoch   1 |   800/ 1000 batches | lr 0.24 | ms/batch  0.65 | loss  0.16 | ppl     1.18\n",
      "| epoch   1 |   900/ 1000 batches | lr 0.24 | ms/batch  0.59 | loss  0.15 | ppl     1.17\n",
      "| epoch   1 |  1000/ 1000 batches | lr 0.24 | ms/batch  0.59 | loss  0.17 | ppl     1.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  2.22s | valid loss  9.33 | valid ppl 11249.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch   2 |     0/ 1000 batches | lr 0.23 | ms/batch  0.29 | loss  0.08 | ppl     1.08\n",
      "| epoch   2 |   100/ 1000 batches | lr 0.23 | ms/batch  0.59 | loss  0.16 | ppl     1.18\n",
      "| epoch   2 |   200/ 1000 batches | lr 0.23 | ms/batch  0.59 | loss  0.15 | ppl     1.16\n",
      "| epoch   2 |   300/ 1000 batches | lr 0.23 | ms/batch  0.59 | loss  0.15 | ppl     1.17\n",
      "| epoch   2 |   400/ 1000 batches | lr 0.23 | ms/batch  0.69 | loss  0.18 | ppl     1.20\n",
      "| epoch   2 |   500/ 1000 batches | lr 0.23 | ms/batch  0.65 | loss  0.17 | ppl     1.19\n",
      "| epoch   2 |   600/ 1000 batches | lr 0.23 | ms/batch  0.59 | loss  0.15 | ppl     1.16\n",
      "| epoch   2 |   700/ 1000 batches | lr 0.23 | ms/batch  0.61 | loss  0.15 | ppl     1.16\n",
      "| epoch   2 |   800/ 1000 batches | lr 0.23 | ms/batch  0.59 | loss  0.18 | ppl     1.20\n",
      "| epoch   2 |   900/ 1000 batches | lr 0.23 | ms/batch  0.60 | loss  0.15 | ppl     1.16\n",
      "| epoch   2 |  1000/ 1000 batches | lr 0.23 | ms/batch  0.57 | loss  0.18 | ppl     1.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  2.25s | valid loss 10.77 | valid ppl 47505.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch   3 |     0/ 1000 batches | lr 0.21 | ms/batch  0.31 | loss  0.09 | ppl     1.10\n",
      "| epoch   3 |   100/ 1000 batches | lr 0.21 | ms/batch  0.66 | loss  0.21 | ppl     1.24\n",
      "| epoch   3 |   200/ 1000 batches | lr 0.21 | ms/batch  0.59 | loss  0.18 | ppl     1.20\n",
      "| epoch   3 |   300/ 1000 batches | lr 0.21 | ms/batch  0.60 | loss  0.18 | ppl     1.20\n",
      "| epoch   3 |   400/ 1000 batches | lr 0.21 | ms/batch  0.61 | loss  0.20 | ppl     1.22\n",
      "| epoch   3 |   500/ 1000 batches | lr 0.21 | ms/batch  0.63 | loss  0.16 | ppl     1.17\n",
      "| epoch   3 |   600/ 1000 batches | lr 0.21 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch   3 |   700/ 1000 batches | lr 0.21 | ms/batch  0.59 | loss  0.16 | ppl     1.18\n",
      "| epoch   3 |   800/ 1000 batches | lr 0.21 | ms/batch  0.60 | loss  0.16 | ppl     1.17\n",
      "| epoch   3 |   900/ 1000 batches | lr 0.21 | ms/batch  0.60 | loss  0.14 | ppl     1.16\n",
      "| epoch   3 |  1000/ 1000 batches | lr 0.21 | ms/batch  0.58 | loss  0.15 | ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  2.12s | valid loss  8.80 | valid ppl  6602.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch   4 |     0/ 1000 batches | lr 0.20 | ms/batch  0.29 | loss  0.08 | ppl     1.08\n",
      "| epoch   4 |   100/ 1000 batches | lr 0.20 | ms/batch  0.57 | loss  0.15 | ppl     1.16\n",
      "| epoch   4 |   200/ 1000 batches | lr 0.20 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch   4 |   300/ 1000 batches | lr 0.20 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch   4 |   400/ 1000 batches | lr 0.20 | ms/batch  0.59 | loss  0.14 | ppl     1.16\n",
      "| epoch   4 |   500/ 1000 batches | lr 0.20 | ms/batch  0.61 | loss  0.14 | ppl     1.15\n",
      "| epoch   4 |   600/ 1000 batches | lr 0.20 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch   4 |   700/ 1000 batches | lr 0.20 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch   4 |   800/ 1000 batches | lr 0.20 | ms/batch  0.59 | loss  0.15 | ppl     1.17\n",
      "| epoch   4 |   900/ 1000 batches | lr 0.20 | ms/batch  0.61 | loss  0.15 | ppl     1.16\n",
      "| epoch   4 |  1000/ 1000 batches | lr 0.20 | ms/batch  0.57 | loss  0.15 | ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  2.12s | valid loss  8.06 | valid ppl  3162.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch   5 |     0/ 1000 batches | lr 0.19 | ms/batch  0.29 | loss  0.09 | ppl     1.09\n",
      "| epoch   5 |   100/ 1000 batches | lr 0.19 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch   5 |   200/ 1000 batches | lr 0.19 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch   5 |   300/ 1000 batches | lr 0.19 | ms/batch  0.60 | loss  0.15 | ppl     1.16\n",
      "| epoch   5 |   400/ 1000 batches | lr 0.19 | ms/batch  0.59 | loss  0.17 | ppl     1.18\n",
      "| epoch   5 |   500/ 1000 batches | lr 0.19 | ms/batch  0.59 | loss  0.15 | ppl     1.17\n",
      "| epoch   5 |   600/ 1000 batches | lr 0.19 | ms/batch  0.59 | loss  0.23 | ppl     1.26\n",
      "| epoch   5 |   700/ 1000 batches | lr 0.19 | ms/batch  0.59 | loss  0.19 | ppl     1.20\n",
      "| epoch   5 |   800/ 1000 batches | lr 0.19 | ms/batch  0.59 | loss  0.15 | ppl     1.16\n",
      "| epoch   5 |   900/ 1000 batches | lr 0.19 | ms/batch  0.58 | loss  0.16 | ppl     1.17\n",
      "| epoch   5 |  1000/ 1000 batches | lr 0.19 | ms/batch  0.57 | loss  0.15 | ppl     1.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  2.11s | valid loss  7.94 | valid ppl  2818.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch   6 |     0/ 1000 batches | lr 0.18 | ms/batch  0.29 | loss  0.08 | ppl     1.08\n",
      "| epoch   6 |   100/ 1000 batches | lr 0.18 | ms/batch  0.57 | loss  0.13 | ppl     1.14\n",
      "| epoch   6 |   200/ 1000 batches | lr 0.18 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch   6 |   300/ 1000 batches | lr 0.18 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch   6 |   400/ 1000 batches | lr 0.18 | ms/batch  0.59 | loss  0.15 | ppl     1.16\n",
      "| epoch   6 |   500/ 1000 batches | lr 0.18 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch   6 |   600/ 1000 batches | lr 0.18 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch   6 |   700/ 1000 batches | lr 0.18 | ms/batch  0.59 | loss  0.15 | ppl     1.16\n",
      "| epoch   6 |   800/ 1000 batches | lr 0.18 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch   6 |   900/ 1000 batches | lr 0.18 | ms/batch  0.61 | loss  0.14 | ppl     1.15\n",
      "| epoch   6 |  1000/ 1000 batches | lr 0.18 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  2.11s | valid loss  7.76 | valid ppl  2343.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch   7 |     0/ 1000 batches | lr 0.17 | ms/batch  0.29 | loss  0.07 | ppl     1.07\n",
      "| epoch   7 |   100/ 1000 batches | lr 0.17 | ms/batch  0.60 | loss  0.13 | ppl     1.14\n",
      "| epoch   7 |   200/ 1000 batches | lr 0.17 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch   7 |   300/ 1000 batches | lr 0.17 | ms/batch  0.57 | loss  0.15 | ppl     1.16\n",
      "| epoch   7 |   400/ 1000 batches | lr 0.17 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch   7 |   500/ 1000 batches | lr 0.17 | ms/batch  0.61 | loss  0.14 | ppl     1.15\n",
      "| epoch   7 |   600/ 1000 batches | lr 0.17 | ms/batch  0.58 | loss  0.15 | ppl     1.16\n",
      "| epoch   7 |   700/ 1000 batches | lr 0.17 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch   7 |   800/ 1000 batches | lr 0.17 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch   7 |   900/ 1000 batches | lr 0.17 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch   7 |  1000/ 1000 batches | lr 0.17 | ms/batch  0.60 | loss  0.15 | ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  2.31s | valid loss  8.07 | valid ppl  3181.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch   8 |     0/ 1000 batches | lr 0.17 | ms/batch  0.29 | loss  0.07 | ppl     1.08\n",
      "| epoch   8 |   100/ 1000 batches | lr 0.17 | ms/batch  0.58 | loss  0.13 | ppl     1.14\n",
      "| epoch   8 |   200/ 1000 batches | lr 0.17 | ms/batch  0.59 | loss  0.13 | ppl     1.14\n",
      "| epoch   8 |   300/ 1000 batches | lr 0.17 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch   8 |   400/ 1000 batches | lr 0.17 | ms/batch  0.62 | loss  0.15 | ppl     1.16\n",
      "| epoch   8 |   500/ 1000 batches | lr 0.17 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch   8 |   600/ 1000 batches | lr 0.17 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch   8 |   700/ 1000 batches | lr 0.17 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch   8 |   800/ 1000 batches | lr 0.17 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch   8 |   900/ 1000 batches | lr 0.17 | ms/batch  0.63 | loss  0.14 | ppl     1.15\n",
      "| epoch   8 |  1000/ 1000 batches | lr 0.17 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  2.12s | valid loss  7.93 | valid ppl  2786.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch   9 |     0/ 1000 batches | lr 0.16 | ms/batch  0.29 | loss  0.07 | ppl     1.07\n",
      "| epoch   9 |   100/ 1000 batches | lr 0.16 | ms/batch  0.59 | loss  0.13 | ppl     1.14\n",
      "| epoch   9 |   200/ 1000 batches | lr 0.16 | ms/batch  0.60 | loss  0.13 | ppl     1.14\n",
      "| epoch   9 |   300/ 1000 batches | lr 0.16 | ms/batch  0.58 | loss  0.14 | ppl     1.16\n",
      "| epoch   9 |   400/ 1000 batches | lr 0.16 | ms/batch  0.62 | loss  0.13 | ppl     1.14\n",
      "| epoch   9 |   500/ 1000 batches | lr 0.16 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch   9 |   600/ 1000 batches | lr 0.16 | ms/batch  0.58 | loss  0.14 | ppl     1.16\n",
      "| epoch   9 |   700/ 1000 batches | lr 0.16 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch   9 |   800/ 1000 batches | lr 0.16 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch   9 |   900/ 1000 batches | lr 0.16 | ms/batch  0.62 | loss  0.14 | ppl     1.15\n",
      "| epoch   9 |  1000/ 1000 batches | lr 0.16 | ms/batch  0.59 | loss  0.15 | ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  2.36s | valid loss  8.15 | valid ppl  3455.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  10 |     0/ 1000 batches | lr 0.15 | ms/batch  0.29 | loss  0.07 | ppl     1.07\n",
      "| epoch  10 |   100/ 1000 batches | lr 0.15 | ms/batch  0.59 | loss  0.13 | ppl     1.14\n",
      "| epoch  10 |   200/ 1000 batches | lr 0.15 | ms/batch  0.60 | loss  0.13 | ppl     1.14\n",
      "| epoch  10 |   300/ 1000 batches | lr 0.15 | ms/batch  0.59 | loss  0.15 | ppl     1.16\n",
      "| epoch  10 |   400/ 1000 batches | lr 0.15 | ms/batch  0.62 | loss  0.14 | ppl     1.15\n",
      "| epoch  10 |   500/ 1000 batches | lr 0.15 | ms/batch  0.64 | loss  0.14 | ppl     1.15\n",
      "| epoch  10 |   600/ 1000 batches | lr 0.15 | ms/batch  0.60 | loss  0.14 | ppl     1.16\n",
      "| epoch  10 |   700/ 1000 batches | lr 0.15 | ms/batch  0.65 | loss  0.14 | ppl     1.15\n",
      "| epoch  10 |   800/ 1000 batches | lr 0.15 | ms/batch  0.71 | loss  0.14 | ppl     1.15\n",
      "| epoch  10 |   900/ 1000 batches | lr 0.15 | ms/batch  0.72 | loss  0.15 | ppl     1.16\n",
      "| epoch  10 |  1000/ 1000 batches | lr 0.15 | ms/batch  0.69 | loss  0.15 | ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  2.29s | valid loss  7.86 | valid ppl  2597.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  11 |     0/ 1000 batches | lr 0.14 | ms/batch  0.31 | loss  0.07 | ppl     1.08\n",
      "| epoch  11 |   100/ 1000 batches | lr 0.14 | ms/batch  0.59 | loss  0.13 | ppl     1.14\n",
      "| epoch  11 |   200/ 1000 batches | lr 0.14 | ms/batch  0.61 | loss  0.14 | ppl     1.15\n",
      "| epoch  11 |   300/ 1000 batches | lr 0.14 | ms/batch  0.59 | loss  0.15 | ppl     1.16\n",
      "| epoch  11 |   400/ 1000 batches | lr 0.14 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  11 |   500/ 1000 batches | lr 0.14 | ms/batch  0.60 | loss  0.15 | ppl     1.16\n",
      "| epoch  11 |   600/ 1000 batches | lr 0.14 | ms/batch  0.59 | loss  0.15 | ppl     1.17\n",
      "| epoch  11 |   700/ 1000 batches | lr 0.14 | ms/batch  0.60 | loss  0.15 | ppl     1.16\n",
      "| epoch  11 |   800/ 1000 batches | lr 0.14 | ms/batch  0.62 | loss  0.15 | ppl     1.16\n",
      "| epoch  11 |   900/ 1000 batches | lr 0.14 | ms/batch  0.59 | loss  0.15 | ppl     1.16\n",
      "| epoch  11 |  1000/ 1000 batches | lr 0.14 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time:  2.12s | valid loss  8.14 | valid ppl  3422.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  12 |     0/ 1000 batches | lr 0.14 | ms/batch  0.29 | loss  0.08 | ppl     1.08\n",
      "| epoch  12 |   100/ 1000 batches | lr 0.14 | ms/batch  0.58 | loss  0.15 | ppl     1.16\n",
      "| epoch  12 |   200/ 1000 batches | lr 0.14 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  12 |   300/ 1000 batches | lr 0.14 | ms/batch  0.62 | loss  0.14 | ppl     1.15\n",
      "| epoch  12 |   400/ 1000 batches | lr 0.14 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  12 |   500/ 1000 batches | lr 0.14 | ms/batch  0.59 | loss  0.15 | ppl     1.16\n",
      "| epoch  12 |   600/ 1000 batches | lr 0.14 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  12 |   700/ 1000 batches | lr 0.14 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  12 |   800/ 1000 batches | lr 0.14 | ms/batch  0.59 | loss  0.15 | ppl     1.16\n",
      "| epoch  12 |   900/ 1000 batches | lr 0.14 | ms/batch  0.67 | loss  0.15 | ppl     1.16\n",
      "| epoch  12 |  1000/ 1000 batches | lr 0.14 | ms/batch  0.67 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time:  2.12s | valid loss  7.72 | valid ppl  2260.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  13 |     0/ 1000 batches | lr 0.13 | ms/batch  0.28 | loss  0.08 | ppl     1.08\n",
      "| epoch  13 |   100/ 1000 batches | lr 0.13 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  13 |   200/ 1000 batches | lr 0.13 | ms/batch  0.60 | loss  0.13 | ppl     1.14\n",
      "| epoch  13 |   300/ 1000 batches | lr 0.13 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  13 |   400/ 1000 batches | lr 0.13 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  13 |   500/ 1000 batches | lr 0.13 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  13 |   600/ 1000 batches | lr 0.13 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  13 |   700/ 1000 batches | lr 0.13 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  13 |   800/ 1000 batches | lr 0.13 | ms/batch  0.59 | loss  0.14 | ppl     1.16\n",
      "| epoch  13 |   900/ 1000 batches | lr 0.13 | ms/batch  0.61 | loss  0.14 | ppl     1.15\n",
      "| epoch  13 |  1000/ 1000 batches | lr 0.13 | ms/batch  0.59 | loss  0.13 | ppl     1.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time:  2.10s | valid loss  7.95 | valid ppl  2848.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  14 |     0/ 1000 batches | lr 0.12 | ms/batch  0.29 | loss  0.07 | ppl     1.07\n",
      "| epoch  14 |   100/ 1000 batches | lr 0.12 | ms/batch  0.59 | loss  0.13 | ppl     1.14\n",
      "| epoch  14 |   200/ 1000 batches | lr 0.12 | ms/batch  0.58 | loss  0.13 | ppl     1.14\n",
      "| epoch  14 |   300/ 1000 batches | lr 0.12 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  14 |   400/ 1000 batches | lr 0.12 | ms/batch  0.61 | loss  0.13 | ppl     1.14\n",
      "| epoch  14 |   500/ 1000 batches | lr 0.12 | ms/batch  0.59 | loss  0.14 | ppl     1.14\n",
      "| epoch  14 |   600/ 1000 batches | lr 0.12 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  14 |   700/ 1000 batches | lr 0.12 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  14 |   800/ 1000 batches | lr 0.12 | ms/batch  0.61 | loss  0.14 | ppl     1.16\n",
      "| epoch  14 |   900/ 1000 batches | lr 0.12 | ms/batch  0.64 | loss  0.14 | ppl     1.15\n",
      "| epoch  14 |  1000/ 1000 batches | lr 0.12 | ms/batch  0.70 | loss  0.13 | ppl     1.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time:  2.12s | valid loss  7.63 | valid ppl  2062.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  15 |     0/ 1000 batches | lr 0.12 | ms/batch  0.28 | loss  0.07 | ppl     1.08\n",
      "| epoch  15 |   100/ 1000 batches | lr 0.12 | ms/batch  0.57 | loss  0.13 | ppl     1.14\n",
      "| epoch  15 |   200/ 1000 batches | lr 0.12 | ms/batch  0.58 | loss  0.13 | ppl     1.14\n",
      "| epoch  15 |   300/ 1000 batches | lr 0.12 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  15 |   400/ 1000 batches | lr 0.12 | ms/batch  0.58 | loss  0.13 | ppl     1.14\n",
      "| epoch  15 |   500/ 1000 batches | lr 0.12 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  15 |   600/ 1000 batches | lr 0.12 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  15 |   700/ 1000 batches | lr 0.12 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  15 |   800/ 1000 batches | lr 0.12 | ms/batch  0.56 | loss  0.14 | ppl     1.15\n",
      "| epoch  15 |   900/ 1000 batches | lr 0.12 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  15 |  1000/ 1000 batches | lr 0.12 | ms/batch  0.56 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time:  2.02s | valid loss  7.61 | valid ppl  2021.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  16 |     0/ 1000 batches | lr 0.11 | ms/batch  0.29 | loss  0.07 | ppl     1.08\n",
      "| epoch  16 |   100/ 1000 batches | lr 0.11 | ms/batch  0.57 | loss  0.13 | ppl     1.14\n",
      "| epoch  16 |   200/ 1000 batches | lr 0.11 | ms/batch  0.57 | loss  0.13 | ppl     1.14\n",
      "| epoch  16 |   300/ 1000 batches | lr 0.11 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  16 |   400/ 1000 batches | lr 0.11 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  16 |   500/ 1000 batches | lr 0.11 | ms/batch  0.58 | loss  0.14 | ppl     1.16\n",
      "| epoch  16 |   600/ 1000 batches | lr 0.11 | ms/batch  0.58 | loss  0.15 | ppl     1.16\n",
      "| epoch  16 |   700/ 1000 batches | lr 0.11 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  16 |   800/ 1000 batches | lr 0.11 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  16 |   900/ 1000 batches | lr 0.11 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  16 |  1000/ 1000 batches | lr 0.11 | ms/batch  0.55 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time:  2.05s | valid loss  7.82 | valid ppl  2491.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  17 |     0/ 1000 batches | lr 0.10 | ms/batch  0.29 | loss  0.07 | ppl     1.08\n",
      "| epoch  17 |   100/ 1000 batches | lr 0.10 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  17 |   200/ 1000 batches | lr 0.10 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  17 |   300/ 1000 batches | lr 0.10 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  17 |   400/ 1000 batches | lr 0.10 | ms/batch  0.59 | loss  0.13 | ppl     1.14\n",
      "| epoch  17 |   500/ 1000 batches | lr 0.10 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  17 |   600/ 1000 batches | lr 0.10 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  17 |   700/ 1000 batches | lr 0.10 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  17 |   800/ 1000 batches | lr 0.10 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  17 |   900/ 1000 batches | lr 0.10 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  17 |  1000/ 1000 batches | lr 0.10 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time:  2.07s | valid loss  7.54 | valid ppl  1888.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  18 |     0/ 1000 batches | lr 0.10 | ms/batch  0.28 | loss  0.07 | ppl     1.07\n",
      "| epoch  18 |   100/ 1000 batches | lr 0.10 | ms/batch  0.57 | loss  0.13 | ppl     1.14\n",
      "| epoch  18 |   200/ 1000 batches | lr 0.10 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  18 |   300/ 1000 batches | lr 0.10 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  18 |   400/ 1000 batches | lr 0.10 | ms/batch  0.57 | loss  0.13 | ppl     1.14\n",
      "| epoch  18 |   500/ 1000 batches | lr 0.10 | ms/batch  0.60 | loss  0.13 | ppl     1.14\n",
      "| epoch  18 |   600/ 1000 batches | lr 0.10 | ms/batch  0.56 | loss  0.14 | ppl     1.15\n",
      "| epoch  18 |   700/ 1000 batches | lr 0.10 | ms/batch  0.58 | loss  0.14 | ppl     1.16\n",
      "| epoch  18 |   800/ 1000 batches | lr 0.10 | ms/batch  0.58 | loss  0.15 | ppl     1.16\n",
      "| epoch  18 |   900/ 1000 batches | lr 0.10 | ms/batch  0.58 | loss  0.15 | ppl     1.16\n",
      "| epoch  18 |  1000/ 1000 batches | lr 0.10 | ms/batch  0.56 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time:  2.02s | valid loss  7.49 | valid ppl  1791.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  19 |     0/ 1000 batches | lr 0.09 | ms/batch  0.28 | loss  0.07 | ppl     1.07\n",
      "| epoch  19 |   100/ 1000 batches | lr 0.09 | ms/batch  0.58 | loss  0.13 | ppl     1.14\n",
      "| epoch  19 |   200/ 1000 batches | lr 0.09 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  19 |   300/ 1000 batches | lr 0.09 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  19 |   400/ 1000 batches | lr 0.09 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  19 |   500/ 1000 batches | lr 0.09 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  19 |   600/ 1000 batches | lr 0.09 | ms/batch  0.56 | loss  0.14 | ppl     1.15\n",
      "| epoch  19 |   700/ 1000 batches | lr 0.09 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  19 |   800/ 1000 batches | lr 0.09 | ms/batch  0.58 | loss  0.15 | ppl     1.16\n",
      "| epoch  19 |   900/ 1000 batches | lr 0.09 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  19 |  1000/ 1000 batches | lr 0.09 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time:  2.08s | valid loss  7.41 | valid ppl  1650.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  20 |     0/ 1000 batches | lr 0.09 | ms/batch  0.28 | loss  0.07 | ppl     1.07\n",
      "| epoch  20 |   100/ 1000 batches | lr 0.09 | ms/batch  0.57 | loss  0.13 | ppl     1.14\n",
      "| epoch  20 |   200/ 1000 batches | lr 0.09 | ms/batch  0.58 | loss  0.13 | ppl     1.14\n",
      "| epoch  20 |   300/ 1000 batches | lr 0.09 | ms/batch  0.60 | loss  0.14 | ppl     1.14\n",
      "| epoch  20 |   400/ 1000 batches | lr 0.09 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  20 |   500/ 1000 batches | lr 0.09 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  20 |   600/ 1000 batches | lr 0.09 | ms/batch  0.60 | loss  0.15 | ppl     1.16\n",
      "| epoch  20 |   700/ 1000 batches | lr 0.09 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  20 |   800/ 1000 batches | lr 0.09 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  20 |   900/ 1000 batches | lr 0.09 | ms/batch  0.59 | loss  0.15 | ppl     1.16\n",
      "| epoch  20 |  1000/ 1000 batches | lr 0.09 | ms/batch  0.56 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time:  2.04s | valid loss  7.65 | valid ppl  2098.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  21 |     0/ 1000 batches | lr 0.09 | ms/batch  0.29 | loss  0.07 | ppl     1.07\n",
      "| epoch  21 |   100/ 1000 batches | lr 0.09 | ms/batch  0.60 | loss  0.13 | ppl     1.14\n",
      "| epoch  21 |   200/ 1000 batches | lr 0.09 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  21 |   300/ 1000 batches | lr 0.09 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  21 |   400/ 1000 batches | lr 0.09 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  21 |   500/ 1000 batches | lr 0.09 | ms/batch  0.59 | loss  0.14 | ppl     1.14\n",
      "| epoch  21 |   600/ 1000 batches | lr 0.09 | ms/batch  0.56 | loss  0.14 | ppl     1.15\n",
      "| epoch  21 |   700/ 1000 batches | lr 0.09 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  21 |   800/ 1000 batches | lr 0.09 | ms/batch  0.58 | loss  0.15 | ppl     1.17\n",
      "| epoch  21 |   900/ 1000 batches | lr 0.09 | ms/batch  0.58 | loss  0.15 | ppl     1.16\n",
      "| epoch  21 |  1000/ 1000 batches | lr 0.09 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time:  2.09s | valid loss  7.71 | valid ppl  2223.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  22 |     0/ 1000 batches | lr 0.08 | ms/batch  0.29 | loss  0.07 | ppl     1.07\n",
      "| epoch  22 |   100/ 1000 batches | lr 0.08 | ms/batch  0.58 | loss  0.13 | ppl     1.14\n",
      "| epoch  22 |   200/ 1000 batches | lr 0.08 | ms/batch  0.58 | loss  0.13 | ppl     1.14\n",
      "| epoch  22 |   300/ 1000 batches | lr 0.08 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  22 |   400/ 1000 batches | lr 0.08 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  22 |   500/ 1000 batches | lr 0.08 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  22 |   600/ 1000 batches | lr 0.08 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  22 |   700/ 1000 batches | lr 0.08 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  22 |   800/ 1000 batches | lr 0.08 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  22 |   900/ 1000 batches | lr 0.08 | ms/batch  0.58 | loss  0.15 | ppl     1.16\n",
      "| epoch  22 |  1000/ 1000 batches | lr 0.08 | ms/batch  0.56 | loss  0.15 | ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time:  2.01s | valid loss  7.58 | valid ppl  1962.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  23 |     0/ 1000 batches | lr 0.08 | ms/batch  0.28 | loss  0.07 | ppl     1.07\n",
      "| epoch  23 |   100/ 1000 batches | lr 0.08 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  23 |   200/ 1000 batches | lr 0.08 | ms/batch  0.59 | loss  0.13 | ppl     1.14\n",
      "| epoch  23 |   300/ 1000 batches | lr 0.08 | ms/batch  0.61 | loss  0.13 | ppl     1.14\n",
      "| epoch  23 |   400/ 1000 batches | lr 0.08 | ms/batch  0.58 | loss  0.15 | ppl     1.16\n",
      "| epoch  23 |   500/ 1000 batches | lr 0.08 | ms/batch  0.57 | loss  0.15 | ppl     1.16\n",
      "| epoch  23 |   600/ 1000 batches | lr 0.08 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  23 |   700/ 1000 batches | lr 0.08 | ms/batch  0.58 | loss  0.14 | ppl     1.14\n",
      "| epoch  23 |   800/ 1000 batches | lr 0.08 | ms/batch  0.57 | loss  0.15 | ppl     1.16\n",
      "| epoch  23 |   900/ 1000 batches | lr 0.08 | ms/batch  0.59 | loss  0.15 | ppl     1.16\n",
      "| epoch  23 |  1000/ 1000 batches | lr 0.08 | ms/batch  0.56 | loss  0.15 | ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time:  2.03s | valid loss  7.65 | valid ppl  2090.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  24 |     0/ 1000 batches | lr 0.07 | ms/batch  0.28 | loss  0.07 | ppl     1.07\n",
      "| epoch  24 |   100/ 1000 batches | lr 0.07 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  24 |   200/ 1000 batches | lr 0.07 | ms/batch  0.59 | loss  0.14 | ppl     1.14\n",
      "| epoch  24 |   300/ 1000 batches | lr 0.07 | ms/batch  0.56 | loss  0.14 | ppl     1.15\n",
      "| epoch  24 |   400/ 1000 batches | lr 0.07 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  24 |   500/ 1000 batches | lr 0.07 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  24 |   600/ 1000 batches | lr 0.07 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  24 |   700/ 1000 batches | lr 0.07 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  24 |   800/ 1000 batches | lr 0.07 | ms/batch  0.58 | loss  0.15 | ppl     1.16\n",
      "| epoch  24 |   900/ 1000 batches | lr 0.07 | ms/batch  0.61 | loss  0.15 | ppl     1.16\n",
      "| epoch  24 |  1000/ 1000 batches | lr 0.07 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time:  2.12s | valid loss  7.19 | valid ppl  1324.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  25 |     0/ 1000 batches | lr 0.07 | ms/batch  0.29 | loss  0.07 | ppl     1.07\n",
      "| epoch  25 |   100/ 1000 batches | lr 0.07 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  25 |   200/ 1000 batches | lr 0.07 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  25 |   300/ 1000 batches | lr 0.07 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  25 |   400/ 1000 batches | lr 0.07 | ms/batch  0.57 | loss  0.15 | ppl     1.16\n",
      "| epoch  25 |   500/ 1000 batches | lr 0.07 | ms/batch  0.58 | loss  0.15 | ppl     1.16\n",
      "| epoch  25 |   600/ 1000 batches | lr 0.07 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  25 |   700/ 1000 batches | lr 0.07 | ms/batch  0.57 | loss  0.13 | ppl     1.14\n",
      "| epoch  25 |   800/ 1000 batches | lr 0.07 | ms/batch  0.57 | loss  0.13 | ppl     1.14\n",
      "| epoch  25 |   900/ 1000 batches | lr 0.07 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  25 |  1000/ 1000 batches | lr 0.07 | ms/batch  0.56 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time:  2.15s | valid loss  7.51 | valid ppl  1829.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  26 |     0/ 1000 batches | lr 0.07 | ms/batch  0.29 | loss  0.07 | ppl     1.07\n",
      "| epoch  26 |   100/ 1000 batches | lr 0.07 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  26 |   200/ 1000 batches | lr 0.07 | ms/batch  0.59 | loss  0.13 | ppl     1.14\n",
      "| epoch  26 |   300/ 1000 batches | lr 0.07 | ms/batch  0.58 | loss  0.13 | ppl     1.14\n",
      "| epoch  26 |   400/ 1000 batches | lr 0.07 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  26 |   500/ 1000 batches | lr 0.07 | ms/batch  0.59 | loss  0.14 | ppl     1.16\n",
      "| epoch  26 |   600/ 1000 batches | lr 0.07 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  26 |   700/ 1000 batches | lr 0.07 | ms/batch  0.57 | loss  0.14 | ppl     1.14\n",
      "| epoch  26 |   800/ 1000 batches | lr 0.07 | ms/batch  0.59 | loss  0.14 | ppl     1.14\n",
      "| epoch  26 |   900/ 1000 batches | lr 0.07 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  26 |  1000/ 1000 batches | lr 0.07 | ms/batch  0.56 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time:  2.11s | valid loss  7.73 | valid ppl  2279.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  27 |     0/ 1000 batches | lr 0.06 | ms/batch  0.29 | loss  0.07 | ppl     1.07\n",
      "| epoch  27 |   100/ 1000 batches | lr 0.06 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  27 |   200/ 1000 batches | lr 0.06 | ms/batch  0.60 | loss  0.13 | ppl     1.14\n",
      "| epoch  27 |   300/ 1000 batches | lr 0.06 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  27 |   400/ 1000 batches | lr 0.06 | ms/batch  0.58 | loss  0.13 | ppl     1.14\n",
      "| epoch  27 |   500/ 1000 batches | lr 0.06 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  27 |   600/ 1000 batches | lr 0.06 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  27 |   700/ 1000 batches | lr 0.06 | ms/batch  0.57 | loss  0.13 | ppl     1.14\n",
      "| epoch  27 |   800/ 1000 batches | lr 0.06 | ms/batch  0.59 | loss  0.13 | ppl     1.14\n",
      "| epoch  27 |   900/ 1000 batches | lr 0.06 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  27 |  1000/ 1000 batches | lr 0.06 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time:  2.27s | valid loss  7.96 | valid ppl  2862.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  28 |     0/ 1000 batches | lr 0.06 | ms/batch  0.30 | loss  0.07 | ppl     1.07\n",
      "| epoch  28 |   100/ 1000 batches | lr 0.06 | ms/batch  0.61 | loss  0.13 | ppl     1.14\n",
      "| epoch  28 |   200/ 1000 batches | lr 0.06 | ms/batch  0.63 | loss  0.13 | ppl     1.14\n",
      "| epoch  28 |   300/ 1000 batches | lr 0.06 | ms/batch  0.63 | loss  0.13 | ppl     1.14\n",
      "| epoch  28 |   400/ 1000 batches | lr 0.06 | ms/batch  0.61 | loss  0.13 | ppl     1.14\n",
      "| epoch  28 |   500/ 1000 batches | lr 0.06 | ms/batch  0.64 | loss  0.14 | ppl     1.15\n",
      "| epoch  28 |   600/ 1000 batches | lr 0.06 | ms/batch  0.65 | loss  0.13 | ppl     1.14\n",
      "| epoch  28 |   700/ 1000 batches | lr 0.06 | ms/batch  0.60 | loss  0.13 | ppl     1.14\n",
      "| epoch  28 |   800/ 1000 batches | lr 0.06 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  28 |   900/ 1000 batches | lr 0.06 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  28 |  1000/ 1000 batches | lr 0.06 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time:  2.20s | valid loss  7.72 | valid ppl  2262.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  29 |     0/ 1000 batches | lr 0.06 | ms/batch  0.32 | loss  0.07 | ppl     1.07\n",
      "| epoch  29 |   100/ 1000 batches | lr 0.06 | ms/batch  0.59 | loss  0.13 | ppl     1.14\n",
      "| epoch  29 |   200/ 1000 batches | lr 0.06 | ms/batch  0.65 | loss  0.13 | ppl     1.14\n",
      "| epoch  29 |   300/ 1000 batches | lr 0.06 | ms/batch  0.63 | loss  0.14 | ppl     1.15\n",
      "| epoch  29 |   400/ 1000 batches | lr 0.06 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  29 |   500/ 1000 batches | lr 0.06 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  29 |   600/ 1000 batches | lr 0.06 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  29 |   700/ 1000 batches | lr 0.06 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  29 |   800/ 1000 batches | lr 0.06 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  29 |   900/ 1000 batches | lr 0.06 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  29 |  1000/ 1000 batches | lr 0.06 | ms/batch  0.56 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time:  2.06s | valid loss  7.49 | valid ppl  1798.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  30 |     0/ 1000 batches | lr 0.05 | ms/batch  0.28 | loss  0.07 | ppl     1.07\n",
      "| epoch  30 |   100/ 1000 batches | lr 0.05 | ms/batch  0.57 | loss  0.13 | ppl     1.14\n",
      "| epoch  30 |   200/ 1000 batches | lr 0.05 | ms/batch  0.58 | loss  0.13 | ppl     1.14\n",
      "| epoch  30 |   300/ 1000 batches | lr 0.05 | ms/batch  0.56 | loss  0.14 | ppl     1.15\n",
      "| epoch  30 |   400/ 1000 batches | lr 0.05 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  30 |   500/ 1000 batches | lr 0.05 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  30 |   600/ 1000 batches | lr 0.05 | ms/batch  0.57 | loss  0.13 | ppl     1.14\n",
      "| epoch  30 |   700/ 1000 batches | lr 0.05 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  30 |   800/ 1000 batches | lr 0.05 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  30 |   900/ 1000 batches | lr 0.05 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  30 |  1000/ 1000 batches | lr 0.05 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time:  2.27s | valid loss  7.47 | valid ppl  1758.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  31 |     0/ 1000 batches | lr 0.05 | ms/batch  0.39 | loss  0.07 | ppl     1.07\n",
      "| epoch  31 |   100/ 1000 batches | lr 0.05 | ms/batch  0.80 | loss  0.13 | ppl     1.14\n",
      "| epoch  31 |   200/ 1000 batches | lr 0.05 | ms/batch  0.71 | loss  0.13 | ppl     1.14\n",
      "| epoch  31 |   300/ 1000 batches | lr 0.05 | ms/batch  0.78 | loss  0.14 | ppl     1.14\n",
      "| epoch  31 |   400/ 1000 batches | lr 0.05 | ms/batch  0.65 | loss  0.13 | ppl     1.14\n",
      "| epoch  31 |   500/ 1000 batches | lr 0.05 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  31 |   600/ 1000 batches | lr 0.05 | ms/batch  0.62 | loss  0.14 | ppl     1.15\n",
      "| epoch  31 |   700/ 1000 batches | lr 0.05 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  31 |   800/ 1000 batches | lr 0.05 | ms/batch  0.60 | loss  0.15 | ppl     1.16\n",
      "| epoch  31 |   900/ 1000 batches | lr 0.05 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  31 |  1000/ 1000 batches | lr 0.05 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time:  2.27s | valid loss  7.24 | valid ppl  1389.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  32 |     0/ 1000 batches | lr 0.05 | ms/batch  0.29 | loss  0.07 | ppl     1.07\n",
      "| epoch  32 |   100/ 1000 batches | lr 0.05 | ms/batch  0.57 | loss  0.13 | ppl     1.14\n",
      "| epoch  32 |   200/ 1000 batches | lr 0.05 | ms/batch  0.58 | loss  0.13 | ppl     1.14\n",
      "| epoch  32 |   300/ 1000 batches | lr 0.05 | ms/batch  0.59 | loss  0.13 | ppl     1.14\n",
      "| epoch  32 |   400/ 1000 batches | lr 0.05 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  32 |   500/ 1000 batches | lr 0.05 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  32 |   600/ 1000 batches | lr 0.05 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  32 |   700/ 1000 batches | lr 0.05 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  32 |   800/ 1000 batches | lr 0.05 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  32 |   900/ 1000 batches | lr 0.05 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  32 |  1000/ 1000 batches | lr 0.05 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time:  2.37s | valid loss  7.32 | valid ppl  1505.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  33 |     0/ 1000 batches | lr 0.05 | ms/batch  0.29 | loss  0.07 | ppl     1.07\n",
      "| epoch  33 |   100/ 1000 batches | lr 0.05 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "| epoch  33 |   200/ 1000 batches | lr 0.05 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  33 |   300/ 1000 batches | lr 0.05 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  33 |   400/ 1000 batches | lr 0.05 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  33 |   500/ 1000 batches | lr 0.05 | ms/batch  0.57 | loss  0.14 | ppl     1.14\n",
      "| epoch  33 |   600/ 1000 batches | lr 0.05 | ms/batch  0.58 | loss  0.13 | ppl     1.14\n",
      "| epoch  33 |   700/ 1000 batches | lr 0.05 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  33 |   800/ 1000 batches | lr 0.05 | ms/batch  0.57 | loss  0.14 | ppl     1.15\n",
      "| epoch  33 |   900/ 1000 batches | lr 0.05 | ms/batch  0.59 | loss  0.14 | ppl     1.15\n",
      "| epoch  33 |  1000/ 1000 batches | lr 0.05 | ms/batch  0.58 | loss  0.14 | ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time:  2.17s | valid loss  7.48 | valid ppl  1770.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "batch size =  2048 ; number of batches =  1000\n",
      "| epoch  34 |     0/ 1000 batches | lr 0.04 | ms/batch  0.30 | loss  0.07 | ppl     1.07\n",
      "| epoch  34 |   100/ 1000 batches | lr 0.04 | ms/batch  0.60 | loss  0.13 | ppl     1.14\n",
      "| epoch  34 |   200/ 1000 batches | lr 0.04 | ms/batch  0.62 | loss  0.13 | ppl     1.14\n",
      "| epoch  34 |   300/ 1000 batches | lr 0.04 | ms/batch  0.63 | loss  0.14 | ppl     1.15\n",
      "| epoch  34 |   400/ 1000 batches | lr 0.04 | ms/batch  0.63 | loss  0.13 | ppl     1.14\n",
      "| epoch  34 |   500/ 1000 batches | lr 0.04 | ms/batch  0.60 | loss  0.14 | ppl     1.15\n",
      "| epoch  34 |   600/ 1000 batches | lr 0.04 | ms/batch  0.58 | loss  0.13 | ppl     1.14\n",
      "| epoch  34 |   700/ 1000 batches | lr 0.04 | ms/batch  0.61 | loss  0.13 | ppl     1.14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, epochs):\n\u001b[0;32m     17\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 19\u001b[0m     train_losses_temp \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     train_losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_losses_temp\n\u001b[0;32m     22\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_data)\n",
      "Cell \u001b[1;32mIn[85], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, verbose)\u001b[0m\n\u001b[0;32m     28\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 31\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     33\u001b[0m     lr \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "best_val_loss = float('inf')\n",
    "epochs = 1000\n",
    "save_model_epochs = 10\n",
    "model, optimizer, scheduler = new_model()\n",
    "# model_file_name = './data/model_epoch_10.pth'\n",
    "# model.load_state_dict(torch.load(model_file_name))\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        train_losses_temp = train(model, verbose=False)\n",
    "        train_losses += train_losses_temp\n",
    "\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_losses += [val_loss]\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "              f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if epoch % save_model_epochs == 0:\n",
    "            # time_stamp = str(time.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
    "            model_file_name = './data/model_epoch_'+str(epoch)+'.pth'\n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "            train_losses_file_name = './data/train_losses_epoch'+str(epoch)+'.csv'\n",
    "            train_loss_out = np.array(train_losses)\n",
    "            train_loss_out.tofile(train_losses_file_name, sep=',', format='%0.4f')\n",
    "            val_losses_file_name = './data/val_losses_epoch'+str(epoch)+'.csv'\n",
    "            val_loss_out = np.array(val_losses)\n",
    "            val_loss_out.tofile(val_losses_file_name, sep=',', format='%0.4f')\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b3eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.083 0.252 0.179 0.073 0.088 0.1   0.109 0.096 0.085 0.081 0.075 0.086\n",
      " 0.081 0.087 0.071 0.066 0.08  0.077 0.086 0.084 0.074 0.085 0.092 0.086\n",
      " 0.081 0.084 0.089 0.091 0.078 0.075 0.077 0.075 0.082 0.084 0.09  0.091\n",
      " 0.083 0.075 0.074 0.078 0.082 0.085 0.085 0.079 0.078 0.076 0.083 0.084\n",
      " 0.083 0.074 0.067 0.065 0.072 0.073 0.073 0.08  0.076 0.075 0.07  0.069\n",
      " 0.069 0.069 0.068 0.074 0.079 0.079]\n"
     ]
    }
   ],
   "source": [
    "train_losses_file_name = './'+'train_losses_epoch'+str(epoch)+'.csv'\n",
    "lossesscv = np.array(train_losses).round(3)\n",
    "print(lossesscv)\n",
    "# np.savetxt(train_losses_file_name, lossesscv, delimiter=\",\")\n",
    "lossesscv.tofile(train_losses_file_name,sep=',',format='%0.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c4edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.083, 0.252, 0.179, 0.073, 0.088, 0.1  , 0.109, 0.096, 0.085,\n",
       "       0.081, 0.075, 0.086, 0.081, 0.087, 0.071, 0.066, 0.08 , 0.077,\n",
       "       0.086, 0.084, 0.074, 0.085, 0.092, 0.086, 0.081, 0.084, 0.089,\n",
       "       0.091, 0.078, 0.075, 0.077, 0.075, 0.082, 0.084, 0.09 , 0.091,\n",
       "       0.083, 0.075, 0.074, 0.078, 0.082, 0.085, 0.085, 0.079, 0.078,\n",
       "       0.076, 0.083, 0.084, 0.083, 0.074, 0.067, 0.065, 0.072, 0.073,\n",
       "       0.073, 0.08 , 0.076, 0.075, 0.07 , 0.069, 0.069, 0.069, 0.068,\n",
       "       0.074, 0.079, 0.079])"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_losses).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d19eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3739267ee6d59cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T05:01:14.977250300Z",
     "start_time": "2023-12-22T05:01:14.977250300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  8.70 | test ppl  6020.76\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c15b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "tensor([   9, 3849, 3869,  ..., 2442, 4810,    3], device='cuda:0')\n",
      "torch.Size([2049990])\n",
      "9.187452700594196\n",
      "Validation:\n",
      "tensor([    9,  9606, 25610,  ...,     9,     9,     9], device='cuda:0')\n",
      "torch.Size([214417])\n",
      "8.768819147130907\n",
      "Test:\n",
      "tensor([   9,  632,    0,  ..., 7213,    0,    3], device='cuda:0')\n",
      "torch.Size([241859])\n",
      "8.702968146449683\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\")\n",
    "print(train_data)\n",
    "print(train_data.shape)\n",
    "print(evaluate(model, train_data))\n",
    "\n",
    "print(\"Validation:\")\n",
    "print(val_data)\n",
    "print(val_data.shape)\n",
    "print(evaluate(model, val_data))\n",
    "\n",
    "print(\"Test:\")\n",
    "print(test_data)\n",
    "print(test_data.shape)\n",
    "print(evaluate(model, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144189e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 12, 30, 2, 2, 48, 741089)"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5960eb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-30_02:02:48_epoch\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time_stamp = time.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "fname = time_stamp+'_epoch'\n",
    "print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c37d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-30_02:02:48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(time_stamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8eca12988a9a99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T06:08:30.107239300Z",
     "start_time": "2023-12-22T06:08:30.069221100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4070 Laptop GPU'"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.current_device()     # The ID of the current GPU.\n",
    "torch.cuda.get_device_name()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
