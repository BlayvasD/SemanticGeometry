{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TransformerModel import *\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import dataset\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size, seq_len:  20 102499\n",
      "batch size, seq_len:  10 21441\n",
      "batch size, seq_len:  10 24185\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english') # Will try spacy\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# ``train_iter`` was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape ``[seq_len, batch_size]``\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_process(train_iter)\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "val_data = data_process(val_iter)\n",
    "val_data = val_data.to(device)\n",
    "\n",
    "test_data = data_process(test_iter)\n",
    "test_data = test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_text(data):\n",
    "    words = [vocab.get_itos()[token] for token in data]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 50  # embedding dimension (default 200)\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = './data/model_epoch_50.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
    "model.load_state_dict(torch.load(model_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.813818388808602"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter_list = list(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  9, 632,   0,  ...,  41,   4,   1], device='cuda:0')\n",
      "torch.Size([2048])\n",
      "tensor([ 632,    0,    9,  ...,    4,    1, 2889], device='cuda:0')\n",
      "torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "data, targets = get_batch(test_data, 0)\n",
    "print(data)\n",
    "print(data.shape)\n",
    "print(targets)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"= robert <unk> = robert <unk> is an english film , television and theatre actor . he had a guest @-@ starring role on the television series the bill in 2000 . this was followed by a starring role in the play herons written by simon stephens , which was performed in 2001 at the royal court theatre . he had a guest role in the television series judge john <unk> in 2002 . in 2004 <unk> landed a role as craig in the episode teddy ' s story of the television series the long firm he starred alongside actors mark strong and derek jacobi . he was cast in the 2005 theatre productions of the philip ridley play mercury fur , which was performed at the drum theatre in plymouth and the <unk> <unk> factory in london . he was directed by john <unk> and starred alongside ben <unk> , shane <unk> , harry kent , fraser <unk> , sophie stanton and dominic hall . in 2006 , <unk> starred alongside <unk> in the play <unk> written by mark <unk> . he appeared on a 2006 episode of the television series , doctors , followed by a role in the 2007 theatre production of how to curse directed by <unk> <unk> . how to curse was performed at bush theatre in the london borough of <unk> and fulham . <unk> starred in two films in 2008 , <unk> <unk> by filmmaker paris <unk> , and <unk> punch directed by <unk> blackburn . in may 2008 , <unk> made a guest appearance on a two @-@ part episode arc of the television series waking the dead , followed by an appearance on the television series <unk> in november 2008 . he had a recurring role in ten episodes of the television series <unk> in 2010 , as <unk> fletcher . <unk> starred in the 2011 film <unk> directed by paris <unk> . = = career = = = = = 2000 – 2005 = = = in 2000 <unk> had a guest @-@ starring role on the television series the bill he portrayed scott parry in the episode , in safe hands . <unk> starred as scott in the play herons written by simon stephens , which was performed in 2001 at the royal court theatre . a review of <unk> ' s performance in the independent on sunday described him as horribly menacing in the role , and he received critical reviews in the herald , and evening standard . he appeared in the television series judge john <unk> in 2002 as <unk> <unk> in the episode political <unk> , and had a role as a different character toby steele on the bill . he had a recurring role in 2003 on two episodes of the bill , as character connor price . in 2004 <unk> landed a role as craig in the episode teddy ' s story of the television series the long firm he starred alongside actors mark strong and derek jacobi . <unk> starred as darren , in the 2005 theatre productions of the philip ridley play mercury fur . it was performed at the drum theatre in plymouth , and the <unk> <unk> factory in london . he was directed by john <unk> and starred alongside ben <unk> , shane <unk> , harry kent , fraser <unk> , sophie stanton and dominic hall . <unk> received a favorable review in the daily telegraph the acting is <unk> intense , with <unk> performances from ben <unk> ( now <unk> from his performance as trevor <unk> ' s hamlet ) , robert <unk> , shane <unk> and fraser <unk> . the guardian noted , ben <unk> and robert <unk> offer <unk> amid the <unk> . = = = 2006 – present = = = in 2006 <unk> starred in the play <unk> written by mark <unk> . the play was part of a series which featured different <unk> , titled burn / <unk> / <unk> . in a 2006 interview , fellow actor ben <unk> identified <unk> as one of his favorite co @-@ stars i loved working with a guy called robert <unk> , who was in the triple bill of burn , <unk> and <unk> at the national . he played my brother in mercury fur . he portrayed jason tyler on the 2006 episode of the television series , doctors , titled something i <unk> . <unk> starred as william in the 2007 production of how to curse directed by <unk> <unk> . how to curse was performed at bush theatre in the london borough of <unk> and fulham . in a review of the production for the daily telegraph , theatre critic charles spencer noted , robert <unk> brings a touching vulnerability to the stage as william . <unk> starred in two films in 2008 , <unk> <unk> by filmmaker paris <unk> , and <unk> punch directed by <unk> blackburn . <unk> portrayed a character named sean in <unk> punch , who <unk> along with character josh as the quiet brother . . . who hits it off with <unk> . <unk> guest starred on a two @-@ part episode arc <unk> in may 2008 of the television series waking the dead as character jimmy <unk> . he appeared on the television series <unk> as neil in november 2008 . he had a recurring role in ten episodes of the television series <unk> in 2010 , as <unk> fletcher . he portrayed an emergency physician applying for a medical <unk> . he commented on the inherent difficulties in portraying a physician on television playing a doctor is a strange experience . <unk> you know what you ' re talking about when you don ' t is very bizarre but there are advisers on set who are fantastic at taking you through procedures and giving you the confidence to stand there and look like you know what you ' re doing . <unk> starred in the 2011 film <unk> directed by paris <unk> . = = filmography = = = = = film = = = = = = television = = = = = = theatre = = = = du fu = du fu ( wade – giles tu fu chinese <unk> <unk> – 770 ) was a prominent chinese poet of the tang dynasty . along with li <unk> ( li po ) , he is frequently called the greatest of the chinese poets . his greatest ambition was to serve his country as a successful civil servant , but he proved unable to make the necessary accommodations . his life , like the whole country , was devastated by the an <unk> rebellion of <unk> , and his last 15 years were a time of almost constant unrest . although initially he was little @-@ known to other writers , his works came to be hugely influential in both chinese and japanese literary culture . of his poetic writing , nearly fifteen hundred poems have been preserved over the ages . he has been called the poet @-@ historian and the poet @-@ sage by chinese critics , while the range of his work has allowed him to be introduced to western readers as the chinese virgil , horace , <unk> , shakespeare , milton , burns , <unk> , <unk> , hugo or <unk> . = = life = = traditional chinese literary criticism emphasized the life of the author when interpreting a work , a practice which burton watson attributes to the close links that traditional chinese thought posits between art and morality . since many of du fu ' s poems feature morality and history , this practice is particularly important . another reason , identified by the chinese historian william <unk> , is that chinese poems are typically <unk> , <unk> context that might be relevant , but which an informed contemporary could be assumed to know . for modern western readers , the less accurately we know the time , the place and the circumstances in the background , the more liable we are to imagine it incorrectly , and the result will be that we either <unk> the poem or fail to understand it altogether . stephen owen suggests a third factor particular to du fu , arguing that the variety of the poet ' s work required consideration of his whole life , rather than the <unk> <unk> used for more limited poets . = = = early years = = = most of what is known of du fu ' s life comes from his poems . his paternal grandfather was du <unk> , a noted politician and poet during the reign of empress wu . du fu was born in <unk> the exact birthplace is unknown , except that it was near luoyang , henan province ( gong county is a favourite candidate ) . in later life , he considered himself to belong to the capital city of chang ' an , ancestral hometown of the du family . du fu ' s mother died shortly after he was born , and he was partially raised by his aunt . he had an elder brother , who died young . he also had three half brothers and one half sister , to whom he frequently refers in his poems , although he never mentions his stepmother . the son of a minor scholar @-@ official , his youth was spent on the standard education of a future civil servant study and <unk> of the confucian classics of philosophy , history and poetry . he later claimed to have produced <unk> poems by his early teens , but these have been lost . in the early <unk> , he travelled in the <unk> / zhejiang area his earliest surviving poem , describing a poetry contest , is thought to date from the end of this period , around 735 . in that year , he took the civil service exam , likely in chang ' an . he failed , to his surprise and that of centuries of later critics . <unk> concludes that he probably failed because his prose style at the time was too dense and obscure , while chou suggests his failure to <unk> connections in the capital may have been to blame . after this failure , he went back to traveling , this time around shandong and hebei . his father died around 740 . du fu would have been allowed to enter the civil service because of his father ' s rank , but he is thought to have given up the privilege in favour of one of his half brothers . he spent the next four years living in the luoyang area , fulfilling his duties in domestic affairs . in the autumn of <unk> , he met li <unk> ( li po ) for the first time , and the two poets formed a friendship . david young describes this as the most significant formative element in du fu ' s artistic development because it gave him a living example of the reclusive poet @-@ scholar life to which he was attracted after his failure in the civil service exam . the relationship was somewhat one @-@ sided , however . du fu was by some years the younger , while li <unk> was already a poetic star . we have twelve poems to or about li <unk> from the younger poet , but only one in the other direction . they met again only once , in <unk> . in 746 , he moved to the capital in an attempt to resurrect his official career . he took the civil service exam a second time during the following year , but all the candidates were failed by the prime minister ( apparently in order to prevent the emergence of possible rivals ) . he never again attempted the examinations , instead <unk> the emperor directly in <unk> , 754 and probably again in <unk> . he married around <unk> , and by <unk> the couple had had five children — three sons and two daughters — but one of the\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 28782)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(data).cpu().detach().numpy()\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_tokens = np.argmax(outputs, axis=1)\n",
    "token_to_text(outputs_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_embedding = model.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer_gpt = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model_gpt = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_get_resized_embeddings',\n",
       " '_resize_token_embeddings',\n",
       " 'get_input_embeddings',\n",
       " 'get_output_embeddings',\n",
       " 'get_position_embeddings',\n",
       " 'resize_position_embeddings',\n",
       " 'resize_token_embeddings',\n",
       " 'set_input_embeddings',\n",
       " 'set_output_embeddings']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in dir(model_gpt) if 'embed' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gpt.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=50257, bias=False)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gpt.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([423, 132,   2, 416,  34, 178, 857], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'good day , how are you ?'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token = data_process(['good day, how are you?']).to(device)\n",
    "print(word_token)\n",
    "token_to_text(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1099,  287,  240,  868], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'queen king man woman'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token = data_process(['queen king man woman']).to(device)\n",
    "print(word_token)\n",
    "token_to_text(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28782, 50])\n"
     ]
    }
   ],
   "source": [
    "vocab_indices = torch.arange(len(vocab)).to(device)\n",
    "vocab_embedding = my_embedding(vocab_indices)\n",
    "print(vocab_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances(vector, vocab_embedding=vocab_embedding):\n",
    "    expanded_vector = vector.unsqueeze(0).expand_as(vocab_embedding)\n",
    "    euclidean_distances = torch.sqrt(torch.sum((expanded_vector - vocab_embedding) ** 2, dim=1))\n",
    "    return euclidean_distances.tolist()\n",
    "\n",
    "def top_n_closest(distances):\n",
    "    distances_tensor = torch.tensor(distances)\n",
    "    smallest_distances, indices = torch.topk(distances_tensor, 10, largest=False)\n",
    "    return smallest_distances.tolist(), indices.tolist()\n",
    "\n",
    "def print_closest(vector, n=10, vocab_embedding=vocab_embedding):\n",
    "    distances = get_distances(vector)\n",
    "    smallest_distances, smallest_indices = top_n_closest(distances)\n",
    "    words = [vocab.get_itos()[token] for token in smallest_indices]\n",
    "    for i in range(n):\n",
    "        print(f\"{words[i]}: {smallest_distances[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "lookup_indices(): incompatible function arguments. The following argument types are supported:\n    1. (self: torchtext._torchtext.Vocab, arg0: list) -> List[int]\n\nInvoked with: <torchtext._torchtext.Vocab object at 0x000001BB3D8737B0>, {'input_ids': [805], 'attention_mask': [1]}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[174], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word_token \u001b[38;5;241m=\u001b[39m \u001b[43mdata_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      2\u001b[0m word_vector \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39membedding(word_token)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m print_closest(word_vector)\n",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m, in \u001b[0;36mdata_process\u001b[1;34m(raw_text_iter)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata_process\u001b[39m(raw_text_iter: dataset\u001b[38;5;241m.\u001b[39mIterableDataset) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts raw text into a flat Tensor.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     data \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(vocab(tokenizer(item)), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m raw_text_iter]\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, data)))\n",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata_process\u001b[39m(raw_text_iter: dataset\u001b[38;5;241m.\u001b[39mIterableDataset) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts raw text into a flat Tensor.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     data \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mvocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m raw_text_iter]\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, data)))\n",
      "File \u001b[1;32mc:\\Users\\ilya\\anaconda3\\envs\\p39n\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ilya\\anaconda3\\envs\\p39n\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ilya\\anaconda3\\envs\\p39n\\lib\\site-packages\\torchtext\\vocab\\vocab.py:35\u001b[0m, in \u001b[0;36mVocab.forward\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mexport\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `lookup_indices` method\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m        The indices associated with a list of `tokens`.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: lookup_indices(): incompatible function arguments. The following argument types are supported:\n    1. (self: torchtext._torchtext.Vocab, arg0: list) -> List[int]\n\nInvoked with: <torchtext._torchtext.Vocab object at 0x000001BB3D8737B0>, {'input_ids': [805], 'attention_mask': [1]}"
     ]
    }
   ],
   "source": [
    "word_token = data_process(['man']).to(device)\n",
    "word_vector = model.embedding(word_token)[0]\n",
    "print_closest(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14: 2.380\n",
      "t: 2.566\n",
      "interest: 2.581\n",
      "funds: 2.615\n",
      "good: 2.636\n",
      "scotland: 2.648\n",
      "rival: 2.651\n",
      "literary: 2.666\n",
      "1867: 2.728\n",
      "chick: 2.741\n"
     ]
    }
   ],
   "source": [
    "king_token = data_process(['king']).to(device)\n",
    "king_vector = my_embedding(king_token)[0]\n",
    "man_token = data_process(['man']).to(device)\n",
    "man_vector = my_embedding(man_token)[0]\n",
    "woman_token = data_process(['woman']).to(device)\n",
    "woman_vector = my_embedding(woman_token)[0]\n",
    "\n",
    "\n",
    "print_closest(king_vector - man_vector + woman_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
